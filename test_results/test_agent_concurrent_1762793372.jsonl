{"ok": true, "elapsed_sec": 6.79, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 10.76, "attempts": 2, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 9.3, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 6.86, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 9.26, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 6.65, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 6.9, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 6.8, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.9, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 6.04, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 6.03, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.58, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.9, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 8.02, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.07, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 4.54, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.9, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 8.72, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.84, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 8.73, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 9.39, "attempts": 2, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 4.92, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 7.71, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.69, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 17.11, "attempts": 2, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.62, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 12.44, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 18.05, "attempts": 2, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 10.11, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 9.49, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 9.44, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 9.4, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 11.39, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 4.87, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 7.86, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 4.94, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 7.68, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 7.67, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 3.99, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 6.93, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 6.71, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 3.72, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 8.45, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 11.63, "attempts": 2, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.45, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 7.93, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 5.02, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 4.25, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 8.57, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
{"ok": true, "elapsed_sec": 4.9, "attempts": 1, "result": "Published: 2025-04-24\nTitle: Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents\nAuthors: Yueying Li, Jim Dai, Tianyi Peng\nSummary: As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.\n\nPublished: 2020-10-12\nTitle: Optimal Scheduling Control in Fluid Models of General $n\\times n$ Input-Queued Switches\nAuthors: Yingdong Lu, Mark S. Squillante, Tonghoon Suk\nSummary: Most of the early input-queued switch research focused on establishing\nthroughput optimality of the max-weight scheduling policy, with some recent\nresearch showing that max-weight scheduling is optimal with respect to total\nexpected delay asymptotically in the heavy-traffic regime. However, the\nquestion of delay-optimal scheduling ..."}
