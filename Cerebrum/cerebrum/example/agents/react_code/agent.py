from cerebrum.llm.apis import (
    llm_chat,
    llm_chat_with_json_output,
)
from dotenv import load_dotenv
from typing import List, Dict, Any
import json

from cerebrum.utils import _parse_json_output
from cerebrum.interface import AutoTool

load_dotenv()

class ReActAgent:
    def __init__(self, on_aios: bool = True):
        self.agent_name = "react"
        self.on_aios = on_aios
        self.max_steps = 5
        self.history_window = 2
        self.history = []

        # ğŸ”§ ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ë§Œë“  CodeTestRunner íˆ´ì„ workerë¡œ ë“±ë¡
        # ToolHub / preloaded ê²½ë¡œ ì´ë¦„ì€ ë„¤ ì„¤ì •ì— ë§ê²Œ ìˆ˜ì •í•´ì¤˜
        self.workers = {
            "code_test_runner": AutoTool.from_preloaded(
                "code/code_test_runner", True
            )
        }

    def run(self, task_input: str) -> Dict[str, Any]:
        llms = [
            {
                "name": "gpt-4o-mini",
                "backend": "openai",
            }
        ]

        # ğŸ”§ LLMì—ê²Œ ì•Œë ¤ì£¼ëŠ” worker ì„¤ëª…
        WORKER_PROMPTS = """
1. Worker name: "code_test_runner"

   Description:
   - Run Python code against test code in an isolated process.
   - Returns whether the tests passed, along with stdout and stderr.

   How to call:
   - Set `worker_name` to "code_test_runner".
   - Set `worker_params` to an object with the following fields:
       - `code` (string): Python source code to be tested.
       - `tests` (string): Python test code that uses the functions/classes in `code`.
       - `timeout` (number, optional): timeout in seconds (default: 5).

   Example:
   {
       "worker_name": "code_test_runner",
       "worker_params": {
           "code": "def add(a, b):\\n    return a + b",
           "tests": "assert add(1, 2) == 3\\nprint('All tests passed!')",
           "timeout": 5.0
       }
   }
"""

        system_prompt = f"""# Task Orchestration Instructions

You are an orchestrator agent responsible for coordinating specialized workers to solve code-related tasks.
Your main worker is a test runner that can execute Python code with tests and report whether the tests pass.

## Main Task
{task_input}

## Available Workers
{WORKER_PROMPTS}

## Your Responsibilities:
1. Analyze the task and decide when to call the code_test_runner worker.
2. Construct correct parameters for the worker (code, tests, timeout).
3. Use the worker's output to understand if the code is correct or needs changes.
4. Synthesize results into a clear explanation for the user.
5. When you have enough information, stop calling workers and finish.

## Solution Requirements:
- Provide brief reasoning before deciding to call a worker.
- Do NOT repeatedly call the worker if you already know the outcome.
- Focus on whether the code passes the tests and why.

## Completion Protocol:
- When you are confident the task is complete, stop calling any worker.
- At that point, set:
  "worker_name": null
  "worker_params": null
- Final answer will be generated by another extraction step from the history.
"""

        messages = [
            {"content": system_prompt, "role": "system"},
        ]

        final_answer = ""
        rounds = 0

        while rounds < self.max_steps:
            step_instructions = f"""
## Step-by-Step Execution Protocol:
Here are the latest {self.history_window} trajectory (at most) you have taken:
<history>
{self.history[-self.history_window:]}
</history>

Your output MUST be a JSON object with the following fields:
- "observation" (string): What you know so far.
- "reasoning" (string): Why you choose the next action.
- "worker_name" (string or null): The worker to call, or null if you are done.
- "worker_params" (object or null): Parameters for the worker, or null if you are done.

IMPORTANT:
- If you believe you ALREADY have enough information to answer the original task,
  you MUST set:
  - "worker_name": null
  - "worker_params": null
- Only call "code_test_runner" when you actually want to execute the code and tests.

Here are two examples of valid outputs:

```json
{{
    "observation": "I have constructed the code and tests for the function.",
    "reasoning": "I should now run the tests to see if the implementation is correct.",
    "worker_name": "code_test_runner",
    "worker_params": {{
        "code": "def add(a, b):\\n    return a + b",
        "tests": "assert add(1, 2) == 3\\nprint('All tests passed!')",
        "timeout": 5.0
    }}
}}

{{
    "observation": "I have already seen that the tests all pass.",
    "reasoning": "I can now summarize the result without calling any more workers.",
    "worker_name": null,
    "worker_params": null
}}
"""
            messages.append({"content": step_instructions, "role": "user"})

            response_format = {
                "type": "json_schema",
                "json_schema": {
                    "name": "orchestration",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "observation": {"type": "string"},
                            "reasoning": {"type": "string"},
                            "worker_name": {
                                "anyOf": [
                                    {"type": "string"},
                                    {"type": "null"},
                                ]
                            },
                            "worker_params": {
                                "anyOf": [
                                    {"type": "object"},
                                    {"type": "null"},
                                ]
                            },
                        },
                        "required": [
                            "observation",
                            "reasoning",
                            "worker_name",
                            "worker_params",
                        ],
                    },
                },
            }

            response = llm_chat_with_json_output(
                agent_name=self.agent_name,
                messages=messages,
                llms=llms,
                response_format=response_format,
            )

            step_response = response["response"]["response_message"]
            resp_dict = _parse_json_output(step_response)

            observation = resp_dict.get("observation", "")
            reasoning = resp_dict.get("reasoning", "")
            worker_name = resp_dict.get("worker_name", None)
            worker_params = resp_dict.get("worker_params", None)

            # ì¢…ë£Œ ì¡°ê±´: worker_nameì´ null
            if worker_name is None:
                trajectory_info = {
                    "round": rounds,
                    "observation": observation,
                    "thought": reasoning,
                    "called_worker": worker_name,
                    "called_worker_params": worker_params,
                    "info": None,
                }
                self.history.append(trajectory_info)

                final_answer = self.get_final_answer(task_input)
                break

            # worker í˜¸ì¶œ ë¶„ê¸°
            if worker_name not in self.workers:
                # LLMì´ ì—‰ëš±í•œ worker_nameì„ ë‚´ë†¨ì„ ë•Œ ë°©ì–´
                trajectory_info = {
                    "round": rounds,
                    "observation": observation,
                    "thought": reasoning,
                    "called_worker": worker_name,
                    "called_worker_params": worker_params,
                    "info": f"Unknown worker: {worker_name}",
                }
                self.history.append(trajectory_info)
                final_answer = self.get_final_answer(task_input)
                break

            # None ë°©ì§€
            if worker_params is None:
                worker_params = {}

            # ğŸ”§ ì—¬ê¸°ì„œ ì‹¤ì œë¡œ test runner íˆ´ì„ ì§ì ‘ ì‹¤í–‰
            worker_response = self.workers[worker_name].run(params=worker_params)
            result = worker_response

            trajectory_info = {
                "round": rounds,
                "observation": observation,
                "thought": reasoning,
                "called_worker": worker_name,
                "called_worker_params": worker_params,
                "info": result,
            }

            print(trajectory_info)
            self.history.append(trajectory_info)

            rounds += 1

        return {
            "agent_name": self.agent_name,
            "result": final_answer,
            "rounds": rounds,
        }

    def get_final_answer(self, task_input: str) -> str:
        """
        Extract the final answer based on the task prompt and current state.
        """
        system_prompt = """
You are an extractor agent. Your job is to extract the final answer
from the history and the task prompt.

You may also use your own general knowledge to produce a helpful answer,
but you should pay attention to what actually happened in the history.
"""

        prompt = f"""
You are solving a code-related task that may involve running tests.

Here is the interaction history:
<history>{self.history}</history>

Here is the original task:
<task>{task_input}</task>

Using both the history AND your own knowledge:
1. Briefly summarize what the test runner did and its results (if any).
2. Then provide a clear final answer or explanation for the user.
"""

        messages = [
            {"content": system_prompt, "role": "system"},
            {"content": prompt, "role": "user"},
        ]

        llms = [
            {
                "name": "gpt-4o-mini",
                "backend": "openai",
            }
        ]

        response = llm_chat(
            agent_name=self.agent_name,
            messages=messages,
            llms=llms,
        )

        return response["response"]["response_message"]


def main():
    agent = ReActAgent()

    # ğŸ”§ ìƒ˜í”Œ ì½”ë“œ + í…ŒìŠ¤íŠ¸: ì¼ë¶€ëŸ¬ ë§ëŠ” ì½”ë“œë¡œ
    sample_code = """
def add(a, b):
    return a + b
"""

    sample_tests = """
assert add(1, 2) == 3
assert add(-1, 5) == 4
print("All tests passed!")
"""

    task = f"""
You are given a Python function implementation and some tests.

<code>
{sample_code}
</code>

<tests>
{sample_tests}
</tests>

Your job:
1. Decide whether to call the code_test_runner worker with this code and tests.
2. Use the worker's output to determine if the implementation is correct.
3. When you are confident, stop calling workers and let the system extract a final answer.
"""

    result = agent.run(task)
    print("=== FINAL RESULT ===")
    print(result["result"])
    print("Rounds:", result["rounds"])


if __name__ == "__main__":
    main()
